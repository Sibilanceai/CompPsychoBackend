import numpy as np
import networkx as nx
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from scipy.cluster.hierarchy import dendrogram, linkage
from scipy.spatial.distance import pdist, squareform
import seaborn as sns
from typing import Dict, List, Tuple, Optional
import logging
import pandas as pd
import json
from pathlib import Path
from collections import defaultdict
import re

# Initialize logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class NarrativeAnalyzer:
    def __init__(self, base_dir: str = None):
        """
        Initialize the NarrativeAnalyzer with the correct directory structure.
        
        Args:
            base_dir: Base directory (defaults to current directory)
        """
        if base_dir is None:
            self.base_dir = Path.cwd()
        else:
            self.base_dir = Path(base_dir)
            
        logger.info(f"Base directory: {self.base_dir}")
        
        # Initialize empty data structures
        self.characters = []
        self.time_series_matrices = {}
        self.time_stamps = {}
        self.final_matrices = {}
        self.transfer_entropy_series = {}
        self.narrative_graphs = []
        self.character_trajectories = defaultdict(list)
        self.graph_data = []  # Initialize empty graph data
        
        # Analysis categories
        self.categories = ['high-level', 'context-specific', 'task-specific']
        self.terms = ['short-term', 'medium-term', 'long-term']
        
        # Try to load characters from various possible locations
        self._load_characters()
        
    def _load_characters(self) -> None:
        """Try to load character list from various possible locations."""
        possible_paths = [
            self.base_dir / 'characters_list.csv',
            self.base_dir.parent / 'data_collection' / 'characters_list.csv',
            self.base_dir / '..' / 'data_collection' / 'characters_list.csv'
        ]
        
        for path in possible_paths:
            try:
                logger.info(f"Trying to load characters from: {path}")
                with open(path, 'r', encoding='utf-8') as f:
                    self.characters = [line.strip().split(',')[0] for line in f if line.strip()]
                logger.info(f"Successfully loaded {len(self.characters)} characters")
                return
            except FileNotFoundError:
                continue
            except Exception as e:
                logger.error(f"Error reading character file {path}: {e}")
                continue
        
        # If we couldn't load characters, try to infer from existing matrix files
        try:
            matrix_files = list(self.base_dir.glob('transition_matrices_*.npy'))
            if matrix_files:
                self.characters = [f.stem.split('_')[-1] for f in matrix_files if 'final' not in f.stem]
                logger.info(f"Inferred characters from matrix files: {self.characters}")
                return
        except Exception as e:
            logger.error(f"Error inferring characters from matrix files: {e}")
        
        logger.warning("No characters loaded - analysis may be limited")

    def load_data(self):
        """Load all necessary data files generated by previous scripts."""
        success = False
        
        # Load matrices and timestamps for each character
        for character in self.characters:
            try:
                # Load time series matrices
                matrices_path = self.base_dir / f'transition_matrices_{character}.npy'
                timestamps_path = self.base_dir / f'time_stamps_{character}.npy'
                final_matrix_path = self.base_dir / f'transition_matrices_final_{character}.npy'
                
                if matrices_path.exists() and timestamps_path.exists():
                    self.time_series_matrices[character] = np.load(matrices_path, allow_pickle=True)
                    self.time_stamps[character] = np.load(timestamps_path, allow_pickle=True)
                    logger.info(f"Loaded time series data for character: {character}")
                    success = True
                
                if final_matrix_path.exists():
                    self.final_matrices[character] = np.load(final_matrix_path, allow_pickle=True)
                    logger.info(f"Loaded final matrix for character: {character}")
                
            except Exception as e:
                logger.error(f"Error loading data for {character}: {e}")
        
        # Load graph data if it exists
        try:
            graph_path = self.base_dir / 'graph_data.json'
            if graph_path.exists():
                with open(graph_path, 'r') as f:
                    self.graph_data = json.load(f)
                logger.info("Loaded graph data successfully")
                success = True
            else:
                logger.warning(f"Graph data file not found at {graph_path}")
        except Exception as e:
            logger.error(f"Error loading graph data: {e}")
        
        if success:
            self._process_graphs()
        else:
            logger.warning("No data was successfully loaded")
        
    def _process_graphs(self):
        """Convert loaded JSON graph data into NetworkX graphs."""
        self.narrative_graphs = []
        
        if not hasattr(self, 'graph_data') or not self.graph_data:
            logger.warning("No graph data available to process")
            return
            
        try:
            for time_index, time_step in enumerate(self.graph_data):
                G = nx.DiGraph()
                
                # Add nodes and edges
                for element in time_step['elements']:
                    if 'position' in element:  # Node
                        G.add_node(
                            element['data']['id'],
                            label=element['data'].get('label', element['data']['id']),
                            group=element['data'].get('group', ''),
                            pos=(element['position']['x'], element['position']['y'])
                        )
                    else:  # Edge
                        G.add_edge(
                            element['data']['source'],
                            element['data']['target'],
                            weight=element['data'].get('weight', 1.0)
                        )
                
                if G.number_of_nodes() == 0:
                    logger.warning(f"Graph at time {time_index} is empty.")
                self.narrative_graphs.append(G)
            
            logger.info(f"Processed {len(self.narrative_graphs)} graphs")
        except Exception as e:
            logger.error(f"Error processing graphs: {e}")
            self.narrative_graphs = []
    
    def analyze_character_dynamics(self) -> Dict:
        """Analyze character interaction dynamics over time."""
        dynamics = {
            'character_centrality': self._analyze_centrality_evolution(),
            'interaction_patterns': self._analyze_interaction_patterns(),
            'community_evolution': self._analyze_community_evolution(),
            'influence_dynamics': self._analyze_influence_dynamics()
        }
        return dynamics
    
    def _analyze_centrality_evolution(self) -> Dict:
        """Track the evolution of character centrality measures."""
        centrality_metrics = defaultdict(lambda: defaultdict(list))
        
        for t, graph in enumerate(self.narrative_graphs):
            if graph.number_of_nodes() == 0:
                logger.warning(f"Graph at time {t} is empty. Skipping centrality computation.")
                continue  # Skip empty graphs

            try:
                degree_cent = nx.degree_centrality(graph)
                betweenness_cent = nx.betweenness_centrality(graph)
                eigenvector_cent = nx.eigenvector_centrality_numpy(graph)
            except nx.NetworkXException as e:
                logger.error(f"Error computing centrality measures at time {t}: {e}")
                continue

            for character in self.characters:
                character_nodes = [node for node in graph.nodes() if character in node]
                if not character_nodes:
                    # If the character's node is not in the graph, append 0.0
                    centrality_metrics[character]['degree'].append(0.0)
                    centrality_metrics[character]['betweenness'].append(0.0)
                    centrality_metrics[character]['eigenvector'].append(0.0)
                else:
                    # If multiple nodes match, average their centrality measures
                    degree_values = [degree_cent.get(node, 0.0) for node in character_nodes]
                    betweenness_values = [betweenness_cent.get(node, 0.0) for node in character_nodes]
                    eigenvector_values = [eigenvector_cent.get(node, 0.0) for node in character_nodes]

                    centrality_metrics[character]['degree'].append(np.mean(degree_values))
                    centrality_metrics[character]['betweenness'].append(np.mean(betweenness_values))
                    centrality_metrics[character]['eigenvector'].append(np.mean(eigenvector_values))
        
        return dict(centrality_metrics)
    
    def _analyze_interaction_patterns(self) -> Dict:
        """Analyze patterns of character interactions over time."""
        interaction_metrics = defaultdict(list)
        
        for t, graph in enumerate(self.narrative_graphs):
            if graph.number_of_nodes() == 0:
                logger.warning(f"Graph at time {t} is empty. Skipping interaction pattern analysis.")
                interaction_metrics['density'].append(0.0)
                interaction_metrics['reciprocity'].append(0.0)
                interaction_metrics['avg_clustering'].append(0.0)
                interaction_metrics['avg_path_length'].append(None)
                continue
            
            # Calculate graph-level metrics
            interaction_metrics['density'].append(nx.density(graph))
            
            # Check if the graph has any edges before computing reciprocity
            if graph.number_of_edges() == 0:
                logger.warning(f"Graph at time {t} has no edges. Reciprocity is undefined.")
                interaction_metrics['reciprocity'].append(0.0)  # Or assign None
            else:
                try:
                    reciprocity = nx.reciprocity(graph)
                    reciprocity = reciprocity if reciprocity is not None else 0.0
                    interaction_metrics['reciprocity'].append(reciprocity)
                except nx.NetworkXException as e:
                    logger.error(f"Error computing reciprocity at time {t}: {e}")
                    interaction_metrics['reciprocity'].append(0.0)
            
            # Calculate clustering coefficients for each time step
            clustering = nx.clustering(graph)
            avg_clustering = np.mean(list(clustering.values())) if clustering else 0.0
            interaction_metrics['avg_clustering'].append(avg_clustering)
            
            # Calculate average path length if graph is strongly connected
            if nx.is_strongly_connected(graph):
                try:
                    avg_path = nx.average_shortest_path_length(graph)
                    interaction_metrics['avg_path_length'].append(avg_path)
                except nx.NetworkXException as e:
                    logger.error(f"Error computing average path length at time {t}: {e}")
                    interaction_metrics['avg_path_length'].append(None)
            else:
                interaction_metrics['avg_path_length'].append(None)
                    
        return dict(interaction_metrics)
    
    def _analyze_community_evolution(self) -> List[Dict]:
        """Track the evolution of character communities over time."""
        community_data = []
        
        for t, graph in enumerate(self.narrative_graphs):
            if graph.number_of_nodes() == 0:
                logger.warning(f"Graph at time {t} is empty. Skipping community detection.")
                community_data.append({
                    'time': t,
                    'num_communities': 0,
                    'membership': {},
                    'modularity': 0.0
                })
                continue  # Skip to the next graph
            
            if graph.number_of_edges() == 0:
                logger.warning(f"Graph at time {t} has no edges. Skipping community detection.")
                community_data.append({
                    'time': t,
                    'num_communities': graph.number_of_nodes(),  # Each node is its own community
                    'membership': {node: [i] for i, node in enumerate(graph.nodes())},
                    'modularity': 0.0
                })
                continue  # Skip to the next graph
            
            # Use Louvain method for community detection
            try:
                communities = nx.community.louvain_communities(graph.to_undirected())
            except Exception as e:
                logger.error(f"Error computing communities at time {t}: {e}")
                communities = []
            
            # Track community membership for each character
            membership = defaultdict(list)
            for i, community in enumerate(communities):
                for node in community:
                    for character in self.characters:
                        if character in node:
                            membership[character].append(i)
            
            try:
                modularity_value = nx.community.modularity(graph.to_undirected(), communities)
            except ZeroDivisionError as e:
                logger.error(f"Division by zero when computing modularity at time {t}: {e}")
                modularity_value = 0.0
            except Exception as e:
                logger.error(f"Error computing modularity at time {t}: {e}")
                modularity_value = 0.0
            
            community_data.append({
                'time': t,
                'num_communities': len(communities),
                'membership': dict(membership),
                'modularity': modularity_value
            })
            
        return community_data
    
    def _analyze_influence_dynamics(self) -> Dict:
        """Analyze patterns of influence between characters based on transfer entropy."""
        influence_patterns = defaultdict(list)
        
        for t, graph in enumerate(self.narrative_graphs):
            # Get character influence scores based on edge weights (transfer entropy)
            influence_scores = {}
            for character in self.characters:
                # Calculate incoming and outgoing influence
                incoming = sum(data['weight'] for _, _, data in graph.in_edges(character, data=True))
                outgoing = sum(data['weight'] for _, _, data in graph.out_edges(character, data=True))
                
                influence_scores[character] = {
                    'incoming_influence': incoming,
                    'outgoing_influence': outgoing,
                    'net_influence': outgoing - incoming
                }
            
            influence_patterns[t] = influence_scores
            
        return dict(influence_patterns)
    
    def compute_narrative_metrics(self) -> Dict:
        """Compute comprehensive narrative structure metrics."""
        character_dynamics = self.analyze_character_dynamics()
        
        metrics = {
            'character_dynamics': character_dynamics,
            'narrative_coherence': self._compute_narrative_coherence(),
            'pacing_analysis': self._analyze_narrative_pacing(),
            'key_moments': self._identify_key_moments()
        }
        return metrics
    
    def _compute_narrative_coherence(self) -> Dict:
        """Measure the overall coherence of the narrative structure."""
        coherence_metrics = {
            'temporal_consistency': [],
            'structural_stability': [],
            'character_integration': []
        }
        
        for t in range(1, len(self.narrative_graphs)):
            prev_graph = self.narrative_graphs[t-1]
            curr_graph = self.narrative_graphs[t]
            
            # Measure temporal consistency
            temporal_consistency = self._compute_graph_similarity(prev_graph, curr_graph)
            coherence_metrics['temporal_consistency'].append(temporal_consistency)
            
            # Measure structural stability
            structural_stability = self._compute_structural_stability(curr_graph)
            coherence_metrics['structural_stability'].append(structural_stability)
            
            # Measure character integration
            char_integration = self._compute_character_integration(curr_graph)
            coherence_metrics['character_integration'].append(char_integration)
            
        return coherence_metrics
    
    def _analyze_narrative_pacing(self) -> Dict:
        """Analyze the pacing of the narrative through network dynamics."""
        pacing_metrics = {
            'interaction_density': [],
            'change_rate': [],
            'tension_indicators': []
        }
        
        for t in range(len(self.narrative_graphs)):
            graph = self.narrative_graphs[t]
            
            # Calculate interaction density
            density = nx.density(graph)
            pacing_metrics['interaction_density'].append(density)
            
            # Calculate rate of change if not first graph
            if t > 0:
                prev_graph = self.narrative_graphs[t-1]
                change_rate = self._compute_graph_change_rate(prev_graph, graph)
                pacing_metrics['change_rate'].append(change_rate)
            
            # Calculate tension indicators
            tension = self._compute_tension_indicators(graph)
            pacing_metrics['tension_indicators'].append(tension)
            
        return pacing_metrics
    
    def _identify_key_moments(self) -> List[Dict]:
        """Identify significant moments in the narrative."""
        key_moments = []
        
        for t in range(1, len(self.narrative_graphs)):
            prev_graph = self.narrative_graphs[t-1]
            curr_graph = self.narrative_graphs[t]
            
            # Detect significant changes in network structure
            change_magnitude = self._compute_graph_change_rate(prev_graph, curr_graph)
            
            if change_magnitude > 0.5:  # Threshold for significant change
                moment = {
                    'time': t,
                    'change_magnitude': change_magnitude,
                    'involved_characters': self._get_involved_characters(curr_graph),
                    'type': self._classify_moment_type(prev_graph, curr_graph)
                }
                key_moments.append(moment)
                
        return key_moments
    
    def visualize_analysis(self, save_dir: Optional[str] = None):
        """Create visualizations of the narrative analysis."""
        if save_dir is None:
            save_dir = self.base_dir / 'narrative_visualizations'
        else:
            save_dir = Path(save_dir)
        
        save_dir.mkdir(exist_ok=True, parents=True)
        logger.info(f"Saving visualizations to: {save_dir}")
        
        if not self.characters:
            logger.error("No character data available for visualization")
            return
            
        self._plot_character_centrality_evolution(save_dir)
        self._plot_interaction_patterns(save_dir)
        self._plot_community_evolution(save_dir)
        self._plot_narrative_coherence(save_dir)
        self._plot_pacing_analysis(save_dir)
        
    def save_analysis_results(self, filepath: Optional[str] = None):
        """Save comprehensive analysis results to JSON."""
        if filepath is None:
            filepath = self.base_dir / 'narrative_analysis_results.json'
            
        results = {
            'narrative_metrics': self.compute_narrative_metrics(),
            'character_dynamics': self.analyze_character_dynamics()
        }
        
        try:
            with open(filepath, 'w') as f:
                json.dump(results, f, indent=4)
            logger.info(f"Analysis results saved to {filepath}")
        except Exception as e:
            logger.error(f"Error saving analysis results: {e}")
            
    # Helper methods for computations
    
    def _compute_graph_similarity(self, g1: nx.DiGraph, g2: nx.DiGraph) -> float:
        """Compute similarity between two graphs."""
        # Use graph edit distance normalized by graph size
        edit_distance = nx.graph_edit_distance(g1, g2, timeout=5)  # 5 second timeout
        if edit_distance is None:  # If timeout occurred
            return self._compute_alternative_similarity(g1, g2)
        max_edges = max(g1.number_of_edges(), g2.number_of_edges())
        return 1 - (edit_distance / (2 * max_edges)) if max_edges > 0 else 0
    
    def _compute_structural_stability(self, graph: nx.DiGraph) -> float:
        """Compute structural stability of the graph."""
        if graph.number_of_nodes() == 0:
            return 0.0

        # Combine multiple structural metrics
        metrics = []

        # Density
        metrics.append(nx.density(graph))

        # Average clustering coefficient
        avg_clustering = nx.average_clustering(graph)
        metrics.append(avg_clustering)

        # Degree correlation
        try:
            degree_correlation = nx.degree_assortativity_coefficient(graph)
            if np.isnan(degree_correlation):
                degree_correlation = 0.0
            metrics.append(abs(degree_correlation))
        except Exception as e:
            logger.error(f"Error computing degree assortativity: {e}")
            metrics.append(0.0)

        return np.mean(metrics)
    
    def _compute_character_integration(self, graph: nx.DiGraph) -> float:
        """Compute how well characters are integrated in the narrative."""
        if graph.number_of_nodes() == 0:
            return 0.0

        # Check if the graph is connected
        if not nx.is_connected(graph.to_undirected()):
            logger.warning("Graph is disconnected. Closeness centrality may be undefined.")
            return 0.0

        # Calculate average closeness centrality
        try:
            closeness_values = nx.closeness_centrality(graph).values()
            return np.mean(list(closeness_values))
        except ZeroDivisionError as e:
            logger.error(f"Division by zero when computing closeness centrality: {e}")
            return 0.0
        except Exception as e:
            logger.error(f"Error computing closeness centrality: {e}")
            return 0.0
            
    def _compute_graph_change_rate(self, g1: nx.DiGraph, g2: nx.DiGraph) -> float:
        """Compute rate of change between two consecutive graphs."""
        # Compute multiple change metrics
        changes = []

        # Node changes
        nodes1, nodes2 = set(g1.nodes()), set(g2.nodes())
        if nodes1 or nodes2:
            node_denominator = max(len(nodes1), len(nodes2))
            if node_denominator == 0:
                node_change = 0.0
            else:
                node_change = len(nodes1.symmetric_difference(nodes2)) / node_denominator
            changes.append(node_change)

        # Edge changes
        edges1, edges2 = set(g1.edges()), set(g2.edges())
        if edges1 or edges2:
            edge_denominator = max(len(edges1), len(edges2))
            if edge_denominator == 0:
                edge_change = 0.0
            else:
                edge_change = len(edges1.symmetric_difference(edges2)) / edge_denominator
            changes.append(edge_change)

        # Weight changes
        common_edges = edges1.intersection(edges2)
        if common_edges:
            weight_changes = []
            for edge in common_edges:
                w1 = g1.edges[edge]['weight']
                w2 = g2.edges[edge]['weight']
                weight_changes.append(abs(w1 - w2))
            if weight_changes:
                changes.append(np.mean(weight_changes))

        return np.mean(changes) if changes else 0.0
    
    def _compute_tension_indicators(self, graph: nx.DiGraph) -> float:
        """Compute narrative tension indicators from graph structure."""
        if graph.number_of_nodes() == 0:
            return 0.0
            
        tension_metrics = []
        
        # Edge density as tension indicator
        tension_metrics.append(nx.density(graph))
        
        # Reciprocity as tension indicator (lower reciprocity might indicate tension)
        if graph.number_of_edges() == 0:
            reciprocity = 0.0
        else:
            try:
                reciprocity = nx.reciprocity(graph)
                reciprocity = reciprocity if reciprocity is not None else 0.0
            except nx.NetworkXException as e:
                logger.error(f"Error computing reciprocity: {e}")
                reciprocity = 0.0
        tension_metrics.append(1 - reciprocity)  # Inverse of reciprocity
        
        # Clustering as tension indicator (lower clustering might indicate tension)
        clustering_values = list(nx.clustering(graph).values())
        if clustering_values:
            avg_clustering = np.mean(clustering_values)
        else:
            avg_clustering = 0.0
        tension_metrics.append(1 - avg_clustering)
        
        return np.mean(tension_metrics) if tension_metrics else 0.0
    
    def _get_involved_characters(self, graph: nx.DiGraph) -> List[str]:
        """Get list of characters involved in significant interactions."""
        significant_characters = []
        
        # Calculate degree centrality
        degree_cent = nx.degree_centrality(graph)
        
        # Get characters with above-average centrality
        avg_centrality = np.mean(list(degree_cent.values()))
        
        for node, centrality in degree_cent.items():
            if centrality > avg_centrality:
                for character in self.characters:
                    if character in node:
                        significant_characters.append(character)
                        
        return list(set(significant_characters))
    
    def _classify_moment_type(self, prev_graph: nx.DiGraph, curr_graph: nx.DiGraph) -> str:
        """Classify the type of narrative moment based on graph changes."""
        # Calculate various change metrics
        density_change = nx.density(curr_graph) - nx.density(prev_graph)
        
        prev_clustering = nx.average_clustering(prev_graph)
        curr_clustering = nx.average_clustering(curr_graph)
        clustering_change = curr_clustering - prev_clustering
        
        # Classify based on changes
        if density_change > 0.3 and clustering_change > 0.3:
            return "convergence"
        elif density_change < -0.3 and clustering_change < -0.3:
            return "divergence"
        elif abs(density_change) > 0.3:
            return "intensity_shift"
        elif abs(clustering_change) > 0.3:
            return "relationship_shift"
        else:
            return "gradual_change"
    
    def _compute_alternative_similarity(self, g1: nx.DiGraph, g2: nx.DiGraph) -> float:
        """Compute alternative graph similarity when edit distance times out."""
        # Compare node sets
        nodes1, nodes2 = set(g1.nodes()), set(g2.nodes())
        node_sim = len(nodes1.intersection(nodes2)) / len(nodes1.union(nodes2)) if nodes1 or nodes2 else 0
        
        # Compare edge sets
        edges1, edges2 = set(g1.edges()), set(g2.edges())
        edge_sim = len(edges1.intersection(edges2)) / len(edges1.union(edges2)) if edges1 or edges2 else 0
        
        # Compare degree distributions
        degrees1 = [d for _, d in g1.degree()]
        degrees2 = [d for _, d in g2.degree()]
        degree_sim = 1 - np.mean(np.abs(np.histogram(degrees1, bins=10)[0] - 
                                      np.histogram(degrees2, bins=10)[0])) if degrees1 and degrees2 else 0
        
        return np.mean([node_sim, edge_sim, degree_sim])
    
    # Visualization methods
    
    def _plot_character_centrality_evolution(self, save_dir: Path):
        """Plot the evolution of character centrality measures."""
        dynamics = self.analyze_character_dynamics()
        centrality_metrics = dynamics['character_centrality']
        
        fig, axes = plt.subplots(3, 1, figsize=(12, 15))
        time_points = range(len(self.narrative_graphs))
        
        metrics = ['degree', 'betweenness', 'eigenvector']
        for idx, metric in enumerate(metrics):
            ax = axes[idx]
            for character in self.characters:
                if character in centrality_metrics:
                    values = centrality_metrics[character][metric]
                    ax.plot(time_points, values, label=character, marker='o')
            
            ax.set_title(f'{metric.capitalize()} Centrality Evolution')
            ax.set_xlabel('Time')
            ax.set_ylabel('Centrality Value')
            ax.legend()
            ax.grid(True)
        
        plt.tight_layout()
        plt.savefig(save_dir / 'character_centrality_evolution.png')
        plt.close()
    
    def _plot_interaction_patterns(self, save_dir: Path):
        """Plot patterns of character interactions over time."""
        interaction_data = self._analyze_interaction_patterns()
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 15))
        time_points = range(len(self.narrative_graphs))
        
        # Plot density evolution
        axes[0, 0].plot(time_points, interaction_data['density'], marker='o')
        axes[0, 0].set_title('Network Density Evolution')
        axes[0, 0].set_xlabel('Time')
        axes[0, 0].set_ylabel('Density')
        
        # Plot reciprocity evolution
        # Handle None values in reciprocity data
        reciprocity_values = [r if r is not None else 0.0 for r in interaction_data['reciprocity']]
        axes[0, 1].plot(time_points, reciprocity_values, marker='o')
        axes[0, 1].set_title('Network Reciprocity Evolution')
        axes[0, 1].set_xlabel('Time')
        axes[0, 1].set_ylabel('Reciprocity')
        
        # Plot clustering evolution
        axes[1, 0].plot(time_points, interaction_data['avg_clustering'], marker='o')
        axes[1, 0].set_title('Average Clustering Evolution')
        axes[1, 0].set_xlabel('Time')
        axes[1, 0].set_ylabel('Average Clustering Coefficient')
        
        # Plot path length evolution
        avg_path_lengths = interaction_data['avg_path_length']
        valid_paths = [p for p in avg_path_lengths if p is not None]
        valid_times = [t for t, p in zip(time_points, avg_path_lengths) if p is not None]
        if valid_times:
            axes[1, 1].plot(valid_times, valid_paths, marker='o')
        else:
            axes[1, 1].text(0.5, 0.5, 'No valid path length data', horizontalalignment='center', verticalalignment='center')
        axes[1, 1].set_title('Average Path Length Evolution')
        axes[1, 1].set_xlabel('Time')
        axes[1, 1].set_ylabel('Average Path Length')
        
        plt.tight_layout()
        plt.savefig(save_dir / 'interaction_patterns.png')
        plt.close()
    
    def _plot_community_evolution(self, save_dir: Path):
        """Plot the evolution of character communities."""
        community_data = self._analyze_community_evolution()
        
        if not community_data:
            logger.warning("No community data available for plotting.")
            return
        
        fig, axes = plt.subplots(2, 1, figsize=(12, 12))
        time_points = [data['time'] for data in community_data]
        
        # Plot number of communities
        num_communities = [data['num_communities'] for data in community_data]
        axes[0].plot(time_points, num_communities, marker='o')
        axes[0].set_title('Number of Communities Evolution')
        axes[0].set_xlabel('Time')
        axes[0].set_ylabel('Number of Communities')
        
        # Plot modularity
        modularity = [data['modularity'] for data in community_data]
        axes[1].plot(time_points, modularity, marker='o')
        axes[1].set_title('Community Modularity Evolution')
        axes[1].set_xlabel('Time')
        axes[1].set_ylabel('Modularity')
        
        plt.tight_layout()
        plt.savefig(save_dir / 'community_evolution.png')
        plt.close()
        
        # Plot community membership heatmap
        if self.characters:
            membership_matrix = np.zeros((len(self.characters), len(community_data)))
            for t_index, data in enumerate(community_data):
                for c_index, character in enumerate(self.characters):
                    if character in data['membership']:
                        membership_matrix[c_index, t_index] = data['membership'][character][0]
                    else:
                        membership_matrix[c_index, t_index] = np.nan  # Use NaN for missing data
            
            plt.figure(figsize=(15, 8))
            sns.heatmap(membership_matrix, xticklabels=time_points, yticklabels=self.characters,
                        cmap='viridis', cbar_kws={'label': 'Community ID'}, mask=np.isnan(membership_matrix))
            plt.title('Character Community Membership Evolution')
            plt.xlabel('Time')
            plt.ylabel('Character')
            plt.savefig(save_dir / 'community_membership.png')
            plt.close()
    
    def _plot_narrative_coherence(self, save_dir: Path):
        """Plot narrative coherence metrics."""
        coherence_metrics = self._compute_narrative_coherence()
        
        fig, axes = plt.subplots(3, 1, figsize=(12, 15))
        time_points = range(1, len(self.narrative_graphs))
        
        metrics = ['temporal_consistency', 'structural_stability', 'character_integration']
        for idx, metric in enumerate(metrics):
            values = coherence_metrics[metric]
            axes[idx].plot(time_points, values, marker='o')
            axes[idx].set_title(f'{metric.replace("_", " ").title()} Evolution')
            axes[idx].set_xlabel('Time')
            axes[idx].set_ylabel('Value')
            axes[idx].grid(True)
        
        plt.tight_layout()
        plt.savefig(save_dir / 'narrative_coherence.png')
        plt.close()
    
    def _plot_pacing_analysis(self, save_dir: Path):
        """Plot narrative pacing analysis."""
        pacing_metrics = self._analyze_narrative_pacing()

        fig, axes = plt.subplots(3, 1, figsize=(12, 15))
        time_points = range(len(self.narrative_graphs))

        # Plot interaction density
        axes[0].plot(time_points, pacing_metrics['interaction_density'], marker='o')
        axes[0].set_title('Interaction Density Evolution')
        axes[0].set_xlabel('Time')
        axes[0].set_ylabel('Density')

        # Plot change rate
        change_times = range(1, len(self.narrative_graphs))
        if pacing_metrics['change_rate']:
            axes[1].plot(change_times, pacing_metrics['change_rate'], marker='o')
        else:
            axes[1].text(0.5, 0.5, 'No change rate data', horizontalalignment='center', verticalalignment='center')
        axes[1].set_title('Narrative Change Rate')
        axes[1].set_xlabel('Time')
        axes[1].set_ylabel('Change Rate')

        # Plot tension indicators
        axes[2].plot(time_points, pacing_metrics['tension_indicators'], marker='o')
        axes[2].set_title('Narrative Tension Indicators')
        axes[2].set_xlabel('Time')
        axes[2].set_ylabel('Tension Value')

        plt.tight_layout()
        plt.savefig(save_dir / 'narrative_pacing.png')
        plt.close()

# Example usage
import traceback

if __name__ == "__main__":
    try:
        # Initialize analyzer with current directory
        analyzer = NarrativeAnalyzer()
        
        # Load data
        analyzer.load_data()
        
        if analyzer.characters:
            # Perform analysis
            metrics = analyzer.compute_narrative_metrics()
            
            # Generate visualizations
            analyzer.visualize_analysis()
            
            # Save results
            analyzer.save_analysis_results()
            
            logger.info("Analysis completed successfully")
        else:
            logger.error("No character data available - analysis cannot proceed")
            
    except Exception as e:
        logger.error(f"Error during analysis: {e}")
        traceback.print_exc()